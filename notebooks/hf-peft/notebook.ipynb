{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter-Efficient Fine-tuning (PEFT) with Low-Level Adaptation (LORA) of a sequence classification base model to perform sequence classification task using HuggingFace PEFT on a single GPU\n",
        "\n",
        "base model task: Sequence-to-Label\n",
        "\n",
        "new model task: Sequence-to-Label"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: python-dotenv==1.0.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.1)\nRequirement already satisfied: scikit-learn==1.5.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.5.1)\nRequirement already satisfied: datasets==2.20.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.20.0)\nRequirement already satisfied: transformers==4.43.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (4.43.2)\nRequirement already satisfied: evaluate==0.4.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.4.2)\nRequirement already satisfied: peft==0.12.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.12.0)\nRequirement already satisfied: sentencepiece==0.2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.2.0)\nRequirement already satisfied: numba==0.60.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (0.60.0)\nRequirement already satisfied: huggingface_hub==0.24.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.24.2)\nRequirement already satisfied: numpy>=1.19.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 2)) (3.5.0)\nRequirement already satisfied: filelock in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.14.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\nRequirement already satisfied: pandas in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (4.66.4)\nRequirement already satisfied: xxhash in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\nRequirement already satisfied: multiprocess in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3)) (2024.5.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.9.5)\nRequirement already satisfied: packaging in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (2024.7.24)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from transformers==4.43.2->-r requirements.txt (line 4)) (0.4.3)\nRequirement already satisfied: psutil in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (5.9.8)\nRequirement already satisfied: torch>=1.13.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (2.4.0)\nRequirement already satisfied: accelerate>=0.21.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from peft==0.12.0->-r requirements.txt (line 6)) (0.33.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from numba==0.60.0->-r requirements.txt (line 8)) (0.43.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from huggingface_hub==0.24.2->-r requirements.txt (line 9)) (4.12.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2024.6.2)\nRequirement already satisfied: sympy in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (1.13.1)\nRequirement already satisfied: networkx in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.2.1)\nRequirement already satisfied: jinja2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.0.3)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (12.5.82)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2024.1)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0->-r requirements.txt (line 3)) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (2.0.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/jupyter_env/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.12.0->-r requirements.txt (line 6)) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input constants\n",
        "import os\n",
        "import dotenv\n",
        "import torch\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "HF_DATASETS_NAME = \"google-research-datasets/go_emotions\"\n",
        "HF_PRETRAINED_MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "DEVICE = 'cpu' # 'cuda' if torch.cuda.is_available() else 'cpu'  # when debugging use 'cpu' for better error messages \n",
        "\n",
        "LORA_R = int(os.getenv('LORA_R'))\n",
        "LORA_ALPHA = int(os.getenv('LORA_ALPHA'))\n",
        "LORA_DROPOUT = float(os.getenv('LORA_DROPOUT'))\n",
        "\n",
        "EPOCHS = int(os.getenv('EPOCHS'))\n",
        "BATCH_SIZE = int(os.getenv('BATCH_SIZE'))\n",
        "LEARNING_RATE = float(os.getenv('LEARNING_RATE'))\n",
        "\n",
        "OUTPUT_DIR = os.path.join('trained', HF_PRETRAINED_MODEL_NAME)\n",
        "HUGGINGFACE_REPO_ID = os.getenv('HUGGINGFACE_REPO_ID')\n",
        "\n",
        "if DEVICE == 'gpu':\n",
        "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1722465876519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"HF pretrained model name: {HF_PRETRAINED_MODEL_NAME}\")\n",
        "print(f\"HF datasets name: {HF_DATASETS_NAME}\")\n",
        "\n",
        "print(f\"LORA r: {LORA_R}\")\n",
        "print(f\"LORA alpha: {LORA_ALPHA}\")\n",
        "print(f\"LORA droupout: {LORA_DROPOUT}\")\n",
        "\n",
        "print(f\"epochs: {EPOCHS}\")\n",
        "print(f\"batch_size: {BATCH_SIZE}\")\n",
        "print(f\"learning rate (lr): {LEARNING_RATE}\")\n",
        "\n",
        "print(f\"Using {DEVICE} device\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "HF pretrained model name: cardiffnlp/twitter-roberta-base-sentiment-latest\nHF datasets name: google-research-datasets/go_emotions\nLORA r: 8\nLORA alpha: 32\nLORA droupout: 0.1\nepochs: 5\nbatch_size: 64\nlearning rate (lr): 0.001\nUsing cpu device\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1722465877726
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Downloading and Loading"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download datasets: train, validation, test\n",
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(HF_DATASETS_NAME)  # doctest: +IGNORE_RESULT"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1722465885446
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "print(f\"datasets: {[k for k in datasets]}\")\n",
        "labelNames = datasets['train'].features['labels'].feature.names\n",
        "labelIds = []\n",
        "for dataset_key in datasets:\n",
        "    print(f\"len({dataset_key}): {len(datasets[dataset_key])}\")\n",
        "    [labelIds.append(l) for ls in datasets[dataset_key]['labels'] for l in ls]\n",
        "labelIds = list(set(labelIds))\n",
        "assert len(labelIds) == len(labelNames)\n",
        "labelIds.sort()\n",
        "print(f\"train dataset: {datasets['train']}\")\n",
        "print(f\"train dataset features: {datasets['train'].features}\")\n",
        "print(f\"labelIds ({len(labelIds)} unique), first 10: {labelIds[:10]}\")\n",
        "print(f\"labelNames ({len(labelNames)} unique), first 10: {labelNames[:10]}\")\n",
        "for i in range(3):\n",
        "    print(f\"Example ({i}): {json.dumps(datasets['train'][i], indent=2)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "datasets: ['train', 'validation', 'test']\nlen(train): 43410\nlen(validation): 5426\nlen(test): 5427\ntrain dataset: Dataset({\n    features: ['text', 'labels', 'id'],\n    num_rows: 43410\n})\ntrain dataset features: {'text': Value(dtype='string', id=None), 'labels': Sequence(feature=ClassLabel(names=['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'], id=None), length=-1, id=None), 'id': Value(dtype='string', id=None)}\nlabelIds (28 unique), first 10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nlabelNames (28 unique), first 10: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment']\nExample (0): {\n  \"text\": \"My favourite food is anything I didn't have to cook myself.\",\n  \"labels\": [\n    27\n  ],\n  \"id\": \"eebbqej\"\n}\nExample (1): {\n  \"text\": \"Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead\",\n  \"labels\": [\n    27\n  ],\n  \"id\": \"ed00q6i\"\n}\nExample (2): {\n  \"text\": \"WHY THE FUCK IS BAYLESS ISOING\",\n  \"labels\": [\n    2\n  ],\n  \"id\": \"eezlygj\"\n}\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1722465904873
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# validate data\n",
        "for key, dataset in datasets.items():\n",
        "    print(key)\n",
        "    for idx, record in enumerate(dataset):\n",
        "        assert len(record['text']) > 0, f\"{key}:{idx} - Expected text, received '{record['text']}'\"\n",
        "        assert len(record['labels']) > 0, f\"{key}:{idx} - Expected labels, received '{record['labels']}'\"\n",
        "        for label in record['labels']:\n",
        "            assert isinstance(label, int), f\"{key}:{idx} - Expected int label, received '{label}'\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Tokenizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# using pipelines\n",
        "from transformers import pipeline\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=HF_PRETRAINED_MODEL_NAME, device=DEVICE)\n",
        "sentiment_task(\"Covid cases are increasing fast!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "[{'label': 'negative', 'score': 0.7235766649246216}]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465909527
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download tokenizer\n",
        "# use_fast: False for Python-based algo when encoding is non-trivial (default), True for Rust-base algo with trivial encoding\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_PRETRAINED_MODEL_NAME, use_fast=False, device=DEVICE)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1722465911788
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "base_config = AutoConfig.from_pretrained(HF_PRETRAINED_MODEL_NAME)\n",
        "base_config"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n  \"architectures\": [\n    \"RobertaForSequenceClassification\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"negative\",\n    \"1\": \"neutral\",\n    \"2\": \"positive\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.43.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465918711
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=HF_PRETRAINED_MODEL_NAME\n",
        ")\n",
        "base_model.to(device=DEVICE)\n",
        "base_model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1722465921367
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=base_model, tokenizer=tokenizer, device=DEVICE)\n",
        "classifier(inputs=\"This is super cool!\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "[{'label': 'positive', 'score': 0.9798469543457031}]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465924436
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test inference\n",
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "from scipy.special import softmax\n",
        "\n",
        "input_text = datasets['train'][0]['text']\n",
        "print(f\"==INPUT TEXT==:\\n{input_text}\")\n",
        "expected_labels = datasets['train'][0]['labels']\n",
        "print(f\"==EXPECTED==:\\n{[f'{labelNames[l]} ({l})' for l in expected_labels]}\")\n",
        "encoded_inputs = tokenizer(input_text, return_tensors='pt').to(device=DEVICE)\n",
        "outputs = base_model(**encoded_inputs)\n",
        "logits = outputs[0][0].detach()\n",
        "scores = softmax(logits.to('cpu'))\n",
        "config = AutoConfig.from_pretrained(HF_PRETRAINED_MODEL_NAME)\n",
        "print(f\"==OUTPUT==:\\n{[{config.id2label[i]: scores[i]} for i in range(len(logits))]}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "==INPUT TEXT==:\nMy favourite food is anything I didn't have to cook myself.\n==EXPECTED==:\n['neutral (27)']\n==OUTPUT==:\n[{'negative': 0.012084135}, {'neutral': 0.06326519}, {'positive': 0.9246506}]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465926507
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning configuration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# download model with target number of labels\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=HF_PRETRAINED_MODEL_NAME,\n",
        "    num_labels=len(labelIds),\n",
        "    ignore_mismatched_sizes=True  # because original model's num_labels < expected model's num_labels \n",
        ")\n",
        "base_model.to(device=DEVICE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([28, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([28]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n  )\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465946377
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of (optionally, trainable or non-embeddings) parameters: {base_model.num_parameters():,}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of (optionally, trainable or non-embeddings) parameters: 124,667,164\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465937668
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the dataset\n",
        "# Hugging Face Transformers models expect tokenized input, rather than a string text.\n",
        "def tokenize_dataset(dataset):\n",
        "    # encode text to input_ids and attention_mask\n",
        "    encoded_text = tokenizer(\n",
        "        text=dataset[\"text\"],\n",
        "        padding='max_length',  # add special padding token to create uniform-length inputs of 'max_length'\n",
        "        truncation=True,  # truncate to 'max_length'\n",
        "        max_length=base_config.max_position_embeddings,\n",
        "        return_tensors='pt')\n",
        "    dataset['input_ids'] = encoded_text.input_ids\n",
        "    dataset['attention_mask'] = encoded_text.attention_mask\n",
        "    # encode labels from List[List[int]] to List[int]\n",
        "    first_labels = []\n",
        "    for labels in dataset['labels']:\n",
        "        first_label = labels[0]\n",
        "        first_labels.append(first_label)\n",
        "    dataset['labels'] = first_labels\n",
        "    return dataset\n",
        "\n",
        "encoded_datasets = datasets.map(\n",
        "    tokenize_dataset, \n",
        "    batched=True,\n",
        "    remove_columns=['id', 'text'])"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1722465955898
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "print(f\"datasets: {[k for k in encoded_datasets]}\")\n",
        "for dataset_key in encoded_datasets:\n",
        "    print(f\"len({dataset_key}): {len(encoded_datasets[dataset_key])}\")\n",
        "print(f\"train dataset: {encoded_datasets['train']}\")\n",
        "print(f\"train dataset features: {encoded_datasets['train'].features}\")\n",
        "for i in range(3):\n",
        "    print(f\"Example ({i}): {json.dumps(encoded_datasets['train'][i])}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "datasets: ['train', 'validation', 'test']\nlen(train): 43410\nlen(validation): 5426\nlen(test): 5427\ntrain dataset: Dataset({\n    features: ['labels', 'input_ids', 'attention_mask'],\n    num_rows: 43410\n})\ntrain dataset features: {'labels': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\nExample (0): {\"labels\": 27, \"input_ids\": [0, 2387, 5548, 689, 16, 932, 38, 399, 75, 33, 7, 7142, 2185, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\nExample (1): {\"labels\": 27, \"input_ids\": [0, 5975, 114, 37, 473, 160, 1003, 6, 961, 40, 206, 36279, 519, 10, 7923, 21927, 154, 19, 82, 1386, 9, 888, 1462, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\nExample (2): {\"labels\": 2, \"input_ids\": [0, 26369, 975, 1941, 46997, 3703, 163, 2547, 43023, 26553, 1862, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1722465956060
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure PEFT\n",
        "# LORA can only target the following module types: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\n",
        "# see LoRA: Low-Rank Adaptation of Large Language Models, Hu et al, 2021: https://arxiv.org/abs/2106.09685\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,  # defines the expected fields of the tokenized dataset\n",
        "    target_modules=['query', 'key', 'value'],  # model modules to apply LoRA to\n",
        "    r=LORA_R, \n",
        "    lora_alpha=LORA_ALPHA, \n",
        "    lora_dropout=LORA_DROPOUT,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1722465958550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap model with PEFT config\n",
        "from peft import get_peft_model\n",
        "\n",
        "peft_wrapped_model = get_peft_model(base_model, peft_config)\n",
        "peft_wrapped_model.print_trainable_parameters()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "trainable params: 1,054,492 || all params: 125,721,656 || trainable%: 0.8388\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1722465960391
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Job"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Transformers for Pytorch"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader/collator to batch input in training and evaluation datasets\n",
        "# DataCollatorWithPadding pads dynamically your text to the length of the longest element in its batch, \n",
        "# so they are a uniform length\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1722465962560
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure evaluation metrics in addition to the default `loss` metric that the `Trainer` computes\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465971808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up the GPU memory\n",
        "if DEVICE == 'gpu':\n",
        "    from numba import cuda\n",
        "    device = cuda.get_current_device()\n",
        "    device.reset()"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465972696
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [OPTIONAL] TROUBLESHOOTING\n",
        "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
        "# To disable this warning, you can either:\n",
        "#\t- Avoid using `tokenizers` before the fork if possible\n",
        "#\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465974118
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train job config\n",
        "# Hugging Face training configuration tools can be used to configure a <T>Trainer.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    #do_train=True,\n",
        "    #do_eval=True,\n",
        "\n",
        "    num_train_epochs=1,    \n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    \n",
        "    weight_decay=0.01,\n",
        "    #gradient_accumulation_steps=2,  # default 1\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    # metric_for_best_model=\"f1\"\n",
        "    \n",
        "    fp16=True,  # lower precision\n",
        "    # use_ipex=True if DEVICE == 'cpu' else False,  # use Intel extension for PyTorch\n",
        "    use_cpu=True if DEVICE == 'cpu' else False  # False will use CUDA or MPS if available\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1722465975520
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [OPTIONAL] TROUBLESHOOTHING\n",
        "# IF\n",
        "# RuntimeError: CUDA error: device-side assert triggered\n",
        "# CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
        "# For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
        "# Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "# IF still errors out try DEVICE = 'cpu' to see error message"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722465977355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The <T>Trainer classes require the user to provide: 1) Metrics 2) A base model 3) A training configuration\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_wrapped_model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_datasets[\"train\"],\n",
        "    eval_dataset=encoded_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1722465978969
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if using GPU, then during training job monitor compute instance in terminal with cli command `nvidia-smi`\n",
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='353' max='5427' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 353/5427 37:08 < 8:56:50, 0.16 it/s, Epoch 0.06/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1722465753706
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "peft_wrapped_model.save_pretrained(OUTPUT_DIR)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721939469374
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save on Huggingface\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "peft_wrapped_model.push_to_hub(\"HUGGINGFACE_REPO_ID\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1721939469382
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}